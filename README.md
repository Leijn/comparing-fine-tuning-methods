# Comparing fine-tuning methods
This project explores different methods of fine-tuning a pre-trained Transformer model DistilBERT for sentiment classification on a movie reviews dataset. The main objective is to compare full fine-tuning, tuning only the classification head, and LoRA-based parameter-efficient fine-tuning in terms of both accuracy and computational efficiency.
