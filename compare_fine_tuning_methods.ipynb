{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZqO9fIUhIMJ"
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SHZOzwsh4fA",
    "outputId": "d70454cd-aa7f-4665-809d-beeb627c411b"
   },
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwicEJPAhvse",
    "outputId": "6959385b-3307-4b93-9fc3-d6ab60ab550f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEtYqHJilXRU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdWf9xImp6Nv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHpwda2GjAmT"
   },
   "outputs": [],
   "source": [
    "train_file = '/content/drive/MyDrive/pa4/train.csv'\n",
    "val_file = '/content/drive/MyDrive/pa4/eval.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTM0Daa9H96H"
   },
   "outputs": [],
   "source": [
    "# Step 1: Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "37d9a6e2b8b34f6ab17d5cc638c5b698",
      "a4406a9e5a914bdba4ce43ac7da13cde",
      "7e69571957e9432cb572f84ba03d0f96",
      "872dd91c52614ecebe5daff3d8c03b9f",
      "1a4daecbcbc44c228344f115fff75b67",
      "dc4e370578404dc2a97073b979b30778",
      "c7f0d0b3a2e2468d9c17b829a9a578a1",
      "726ae925b694462caef118f0f7265bd4",
      "b669ff52931c4e818af9d7103b9b76c7",
      "5e9faf8323fc4919aca5bfc959817a0c",
      "ef7b4af07bb145fcbabbd7a017a4f5c4",
      "e63341742f3a48dcbaed2c36dda2c1da",
      "2a9f0732448444a2be5c6839826eda5b",
      "9cd32f6f9474488a813145b48d0e4b44",
      "a4c0ec310d9945d1ae119e4be4245178",
      "524d8531c7244fc4b34e62d0ad916997",
      "1560294ad2ac442d8458c1449e12d6fd",
      "87e789f065f9498b8ac77aa7ce228674",
      "1e83c8fdb878483da02c6ab00a79cf40",
      "596cc8b630b544a287ed48f16b0cf67c",
      "5b5529132b0d4859bcdae14064fcf312",
      "03018680933c4750a2396996670147d1"
     ]
    },
    "id": "tpgjDjs4mQ-_",
    "outputId": "a57784e9-7e3d-40b8-ab3e-61dfd82b0eb2"
   },
   "outputs": [],
   "source": [
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "cfa18ceac7774c07bdd90f54627f2e07",
      "2df807376a4245de970175561e672bf1",
      "6961862a6c30405081e9624f35002d20",
      "686ca838a8334228a9b4f163a9502794",
      "06a7f034bba94976a381ae3a027af373",
      "f9444a06297c4e93bc04a19ab92fb406",
      "777d29260ec74346b3d960725f5cd01f",
      "e22482edd6254557ab3fe3544db2202d",
      "a459e746a2374514a8b5e64e3045f4c2",
      "7f07c50279054cec9aa7784842be93a2",
      "d4af97190e464b29a0da28af6b4d697c",
      "c9904bc5a7bd43098a66b73c22b4a13f",
      "0ea4ab0bb9fb4e748e2fa054cfdf53a7",
      "2bb89bdc6d7e4911a106b95cbf99c536",
      "789884ef7e8a4adf9e5c247b29f3c91d",
      "303aa7ee6cd645a2931a4c605e816335",
      "fda4e1a053494d13aa9d26b15a2c4fa1",
      "d369c7496d40428ebf374830f174dafa",
      "dce78c5832bb463086cae7f2f3e456e6",
      "11399b24a0304442b2f71c74b794322b",
      "6b41e4e94c374091a4504922ef877c18",
      "acc670686b5f4a1887aa774c573a0e16"
     ]
    },
    "id": "6i3S0s0TnG9H",
    "outputId": "7f723aea-019c-4fda-ab66-79c814af1d17"
   },
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset('csv', data_files = {'train': train_file, 'eval': val_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "85059fb2d5954a1998a107efc2970827",
      "fab9857e13f74edaa08ea928076a1751",
      "5b2a9ada163d4adea961ad2cf753bf37",
      "6e9abede9ae94ea29e379c1ade3f279c",
      "b60ad23bb0fe4e32b063f85404beec5e",
      "272dbc6e0b9442929269125b7e37560e",
      "57da3a465eb640f69309daae157cb624",
      "1b361b4da4934208b2606ac01c907457",
      "50e6712cd3734a07af56f1721eada4a9",
      "c050a9adef1244cd8339ab815a1bcc09",
      "85bed211946245aba346d729719d7f46",
      "685c19c2402a4e6a9cd5bdee643d56f3",
      "53cc81ee19274720b5d338250ec3215e",
      "07be7f506b214331be87819cc626fa4c",
      "7541eb6eaab94e4f8c77c486424e085c",
      "05bef555c2b444f793bcb15e098bfe3f",
      "28542c8ef6b64677976cbfdab339f678",
      "da0d8b30930c4754a199e6bce0dbf5df",
      "18f51f6498514808a83e0a16918cbdaa",
      "4d9f4243f66141d8bf847def28c6eb21",
      "af1fedd3b69044bbaf4439c7fe68d462",
      "21ec27be040f48b6bf2f145bc488f8e9",
      "7d41c478a2cb47a294d9e8df4ee7cd88",
      "de4b8a2f0bca4b4f8033223db8d3f015",
      "fa1dabd073964f8aa73e5a2903f72b53",
      "5c35d4675dad44d3af916e100431d036",
      "42bcd689d8504d7085f9bf0a0a8158d0",
      "da00ea4129ab42f8944b1de97befb3f0",
      "03338568223e428397b712caa2ebb28c",
      "e7eb616aa4b645f78846d73dbde655d4",
      "dc58baf0578a49b0b1fe76ac9072e8dd",
      "656f1ad1d42444778f9a1913fc8702f4",
      "45c6d41d5aab46c79212b76e8e02f505"
     ]
    },
    "id": "0mcYuHrGqVg-",
    "outputId": "8aeaedfe-7278-42b7-b18b-98e58ff04336"
   },
   "outputs": [],
   "source": [
    "# AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "5d7f37cd92e84a078dbc72b5ce5073ac",
      "dca6a2fa01d347c4b6ee881a791aa4ec",
      "a19d126359b34310b7018c1702ad60b2",
      "05c70a8640004510a9afcb7ae8233a64",
      "4ba92b6b09ae406bbe22401a62c3d222",
      "08900010fbec41f4a554b4a9443c787c",
      "b9e6f7468e0745d4834730e89ed23571",
      "2050ff8d739c48488da695cc86cee18c",
      "257f65150afd4bad99cb09bb227f4a40",
      "1d0ef56d22e74709bfa278396cc763b7",
      "d834d0fe949b4794b0f670510bc759b7",
      "4bdc1eb0338540a69e5f2e1afcb907ef",
      "316f6e1fd4064678b308ab9a47ef3b14",
      "fca2bc24bc5242908a33db9a6560336d",
      "0f1b5b221b564bd09f9ab719e6150608",
      "f4aa6e5999a9447281bb2936549d9df8",
      "01ee947dcc164bb3884d36ab7c1518c5",
      "ea0efd3d949441129d4238372136db87",
      "cc03197aa08f419f94c46a065f14a7fa",
      "77cb5e75e4cf421c8baf7f02e669b6ab",
      "2b9f1847fad9442199657326fb763013",
      "62fd0ee6983c4355901e0d07814d54f5"
     ]
    },
    "id": "8n-YxpK9pMcf",
    "outputId": "88c3347a-4b46-42fa-d8d4-24fff4be4263"
   },
   "outputs": [],
   "source": [
    "def tokenize_helper(batch):\n",
    "    return tokenizer(batch['review'], padding=True, truncation=True)\n",
    "tokenized_imdb_dataset = imdb_dataset.map(tokenize_helper, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPEySSOctDAo"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ua4lEQGAtX-f",
    "outputId": "c67c83ac-23f0-429f-ca7c-f37d62dad67c"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4rkxrTXtkD0",
    "outputId": "aa11ace1-b424-46a0-be62-bfce7c70590e"
   },
   "outputs": [],
   "source": [
    "# Counting the number of trainable parameters\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_trainable_params = count_trainable_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyU0Wn85PnwZ"
   },
   "outputs": [],
   "source": [
    "epoch_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CcW9Qz3xdxg",
    "outputId": "87f1a4c0-3678-4436-a10b-379e961d1de6"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=epoch_num,\n",
    "    eval_strategy = 'epoch',\n",
    "    run_name = 'fine_tuned_distilbert'\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AiVIwI4xvdn"
   },
   "outputs": [],
   "source": [
    "# Preparing for training\n",
    "import evaluate\n",
    "\n",
    "accuracy_scorer = evaluate.load('accuracy')\n",
    "\n",
    "def evaluation_helper(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy_scorer.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TRkkdAZx6Cb"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb_dataset['train'],\n",
    "    eval_dataset=tokenized_imdb_dataset['eval'],\n",
    "    compute_metrics=evaluation_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "tpz1Ymqcyphv",
    "outputId": "d934f7e3-e74f-4f47-a6c6-6b242f13f55f"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obyY0O3KyrQW"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine_tuned_distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLGFiIcG2P3B"
   },
   "outputs": [],
   "source": [
    "# Step 2: Tuning the final layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Hs12Jqq2UMB",
    "outputId": "d41f366b-dc71-49d1-fbef-261d49c14142"
   },
   "outputs": [],
   "source": [
    "model_freeze = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "print(model_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fn4Szm5y2-Kh"
   },
   "outputs": [],
   "source": [
    "# Freeze all parameters in the model\n",
    "for param in model_freeze.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification head parameters\n",
    "for name, param in model_freeze.named_parameters():\n",
    "    if \"pre_classifier\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZx0OKHA3Cdh",
    "outputId": "b40a1af7-81fb-4999-a61b-ae52f4a036ba"
   },
   "outputs": [],
   "source": [
    "num_trainable_params_freeze = count_trainable_parameters(model_freeze)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params_freeze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rplOF1v43gJi"
   },
   "outputs": [],
   "source": [
    "training_args_freeze = TrainingArguments(\n",
    "    output_dir='./results_freeze_layers',\n",
    "    num_train_epochs=epoch_num,\n",
    "    eval_strategy='epoch',\n",
    "    run_name='fine_tuned_distilbert_freeze'\n",
    ")\n",
    "\n",
    "trainer_freeze = Trainer(\n",
    "    model=model_freeze,\n",
    "    args=training_args_freeze,\n",
    "    train_dataset=tokenized_imdb_dataset['train'],\n",
    "    eval_dataset=tokenized_imdb_dataset['eval'],\n",
    "    compute_metrics=evaluation_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "EkF8qyJv4Ejm",
    "outputId": "1cea95ad-a274-4a4d-ef5e-7f5a153d343f"
   },
   "outputs": [],
   "source": [
    "trainer_freeze.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20cou85W4Igo"
   },
   "outputs": [],
   "source": [
    "trainer_freeze.save_model(\"./fine_tuned_distilbert_freeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywKemNZp6IHg"
   },
   "outputs": [],
   "source": [
    "# Step 3: Fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UKyo4tg667S"
   },
   "outputs": [],
   "source": [
    "def extract_qv_layers(model):\n",
    "    qv_layers = {}\n",
    "    # Iterate through all Transformer layers in DistilBERT\n",
    "    for i in range(len(model.distilbert.transformer.layer)):\n",
    "        # Get the query and value linear layers\n",
    "        q_name = f'distilbert.transformer.layer.{i}.attention.q_lin'\n",
    "        v_name = f'distilbert.transformer.layer.{i}.attention.v_lin'\n",
    "\n",
    "        q_layer = model.get_submodule(q_name)\n",
    "        v_layer = model.get_submodule(v_name)\n",
    "\n",
    "        # Add them to the dictionary\n",
    "        qv_layers[q_name] = q_layer\n",
    "        qv_layers[v_name] = v_layer\n",
    "\n",
    "    return qv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_0dRd0-8AOY",
    "outputId": "b8da0dbf-efbc-45b1-b512-34bdb2029df1"
   },
   "outputs": [],
   "source": [
    "model_LoRA = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "qv_layers = extract_qv_layers(model_LoRA)\n",
    "print(\"Extracted Q and V layers:\", list(qv_layers.keys()))\n",
    "print(\"The number of layers:\", len(qv_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f3Q62qz8ltX"
   },
   "outputs": [],
   "source": [
    "def replace_layers(model, named_layers):\n",
    "    for name, layer in named_layers.items():\n",
    "        components = name.split('.')\n",
    "        submodule = model\n",
    "        for component in components[:-1]:\n",
    "            submodule = getattr(submodule, component)\n",
    "        setattr(submodule, components[-1], layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObPQYpiWCEfV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Implementing the LoRA layer\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, W, r, alpha):\n",
    "        super().__init__()\n",
    "        self.W = W          # The original linear layer\n",
    "        self.r = r          # Rank of the low-rank approximation\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r  # Scaling factor\n",
    "\n",
    "        self.A = nn.Parameter(torch.empty((r, W.in_features)))  # A : R^r×k\n",
    "        self.B = nn.Parameter(torch.empty((W.out_features, r)))  # B : R^d×r\n",
    "\n",
    "        # Parameter initialization\n",
    "        nn.init.normal_(self.A, mean=0.0, std=0.02)  # Initialize A with normal distribution\n",
    "        nn.init.zeros_(self.B)             # Initialize B with zeros\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"x shape: {x.shape}\")\n",
    "        # print(f\"A shape: {self.A.shape}\")\n",
    "        # print(f\"B shape: {self.B.shape}\")\n",
    "        # print(f\"W shape: {self.W.weight.shape}\")\n",
    "\n",
    "        batch_size, seq_length, in_features = x.shape\n",
    "\n",
    "        # Reshape x for matrix multiplication\n",
    "        x_reshaped = x.view(-1, in_features)  # Shape: (batch_size * seq_length, in_features)\n",
    "\n",
    "        # Compute low-rank update: BAx\n",
    "        lora_update = self.B @ (self.A @ x_reshaped.T)  # Shape: (out_features, batch_size * seq_length)\n",
    "        lora_update = lora_update.T.view(batch_size, seq_length, -1)  # Reshape back to (batch_size, seq_length, out_features)\n",
    "\n",
    "        # Add the low-rank update to the frozen linear layer's output\n",
    "        return self.W(x) + self.scaling * lora_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h_hkMNMJe4o"
   },
   "outputs": [],
   "source": [
    "rank = 64   # Low-rank approximation\n",
    "alpha = 32   # Scaling factor\n",
    "\n",
    "# Wrap each linear layer in the extracted layers with LoRA\n",
    "lora_layers = {\n",
    "    name: LoRALayer(layer, r=rank, alpha=alpha) for name, layer in qv_layers.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wU9gV3aVLnU9",
    "outputId": "a2475e32-ae0b-4d3c-fa65-e69f45ba9218"
   },
   "outputs": [],
   "source": [
    "lora_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qxo8n5GJwea",
    "outputId": "1b60f2b2-f4f6-41de-d3b6-34ddc137b18e"
   },
   "outputs": [],
   "source": [
    "replace_layers(model_LoRA, lora_layers)\n",
    "print(\"Replaced original layers with LoRA layers.\")\n",
    "\n",
    "# Freeze all parameters except LoRA layers\n",
    "for name, param in model_LoRA.named_parameters():\n",
    "    if \"A\" not in name and \"B\" not in name:  # Only keep A and B trainable\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzPePPGNJ1qt",
    "outputId": "babb3fd3-c9bd-4c70-fcac-afac95499be2"
   },
   "outputs": [],
   "source": [
    "num_trainable_params_LoRA = count_trainable_parameters(model_LoRA)\n",
    "print(f\"Number of trainable parameters with LoRA: {num_trainable_params_LoRA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQkpjmyTOjDl"
   },
   "outputs": [],
   "source": [
    "training_args_lora = TrainingArguments(\n",
    "    output_dir='./results_lora',\n",
    "    num_train_epochs=epoch_num,\n",
    "    eval_strategy='epoch',\n",
    "    run_name='fine_tuned_distilbert_lora'\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=model_LoRA,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=tokenized_imdb_dataset['train'],\n",
    "    eval_dataset=tokenized_imdb_dataset['eval'],\n",
    "    compute_metrics=evaluation_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "0qJVseRyOzRg",
    "outputId": "0c5d8a01-b895-4cc7-f60b-5908e024fbcb"
   },
   "outputs": [],
   "source": [
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MC7UsLTAW54R"
   },
   "outputs": [],
   "source": [
    "trainer_lora.save_model(\"./fine_tuned_distilbert_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8_yR_7mKW9b"
   },
   "source": [
    "## Summary\n",
    "*   **Full Fine-Tuning**(Step 1)  achieves the best accuracy but is computationally expensive.\n",
    "*   **Tuning Final Layers Only**(Step 2) is the fastest but achieves lower accuracy, making it suitable for quick prototyping.\n",
    "*   **Fine-Tuning with LoRA**(Step 3) strikes a balance, achieving near full fine-tuning performance and reducing trainable parameters significantly. It is computationally efficient compared to Full Fine-Tuning.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
